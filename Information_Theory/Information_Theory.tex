\documentclass{ctexbook}
\input{D:/Repository/Notebook_by_Leo_Yan/note-setup.tex}
\usepackage{wrapfig}
\usepackage{multicol}
\usepackage{paracol}
\usepackage{varwidth}
\usepackage{dsfont}
\usepackage{fix-cm}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{circuitikz}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes,calc}
\usepackage{standalone}
\usepackage{silence}
\usetikzlibrary{calc}
\allowdisplaybreaks
\raggedbottom
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{graphicx}
\usepackage{grffile}
\graphicspath{{D:/Repository/Notebook_by_Leo_Yan/Figures/}}
\let\cleardoublepage\clearpage
\usepackage{url}

\title{信息论}
\author{Leo Yan}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\clearpage
\chapter{熵，相对熵，互信息}

\section{基本概念}

\begin{definition}[熵]
    设X是离散型随机变量，取值（样本空间）为$\mathcal{X}$，概率分布为$p(x)$，则X的熵(entropy)定义为
    \[H(X) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)=\mathbb{E} _p\left[\log \frac{1}{p(X)}\right]\]
    单位为比特（bit）；若以自然对数为底，则单位为纳特（nat）
    \[H(X)\geq 0;\quad H_b(X)=\log_b a\cdot H_a(X)\]
    也记为$H(p)$
\end{definition}
\begin{remark}
    熵描述随机变量的不确定度；给出了描述随机变量所需的信息量的下界
\end{remark}

\begin{definition}[联合熵]
    设(X,Y)为联合分布的离散型随机变量，则(X,Y)的联合熵(joint entropy)定义为
    \[H(X,Y) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y)=\mathbb{E} _p\left[\log \frac{1}{p(X,Y)}\right]\]
    也记为$H(XY)$
\end{definition}

\begin{definition}[条件熵]
    设(X,Y)为联合分布的离散型随机变量，则在已知Y的条件下X的条件熵(conditional entropy)定义为
    \[H(X|Y) = -\sum_{y \in \mathcal{Y}} p(y) \sum_{x \in \mathcal{X}} p(x|y) \log p(x|y)=\mathbb{E} _p\left[\log \frac{1}{p(X|Y)}\right]\]
\end{definition}

\begin{theorem}[链式法则]
    设(X,Y)为联合分布的离散型随机变量，则有
    \[H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)\]
\end{theorem}
\begin{proof}
    \begin{align*}
        H(X,Y) &= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \\
        &= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x)p(y|x) \log [p(x)p(y|x)] \\
        &= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x) - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(y|x) \\
        &= H(X) + H(Y|X)
    \end{align*}
\end{proof}

\begin{definition}[相对熵]
    设p和q为定义在同一样本空间$\mathcal{X}$上的两个离散概率分布，则p相对于q的相对熵(relative entropy)或Kullback-Leibler散度(KL散度)定义为
    \[D(p||q) = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = \mathbb{E} _p\left[\log \frac{p(X)}{q(X)}\right]\]
\end{definition}
\begin{remark}
    描述两个概率分布之间的差异；真实分布为p，假设分布为q的无效性
\end{remark}

\begin{definition}[互信息]
    设(X,Y)为联合分布的离散型随机变量，则X和Y的互信息(mutual information)定义为
    \[I(X;Y) = D(p(x,y)||p(x)p(y)) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} = \mathbb{E} _p\left[\log \frac{p(X,Y)}{p(X)p(Y)}\right]\]
\end{definition}
\begin{remark}
    描述两个随机变量之间共享的信息量，或X包含Y的信息量
\end{remark}

\begin{proposition}[互信息的性质]
    设(X,Y)为联合分布的离散型随机变量，则有
    \begin{enumerate}
        \item $I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$
        \item $I(X;Y) = H(X) + H(Y) - H(X,Y)$
        \item $I(X;X) = H(X)$
    \end{enumerate}
\end{proposition}
\begin{remark}
    以上三条性质的直观：
    \begin{itemize}
        \item 给定X，Y的不确定性减少了$I(X;Y)$
        \item 容斥原理
        \item 熵又是自信息(self-information)
    \end{itemize}
\end{remark}

\section{多维随机变量}

\textbf{符号说明}

(1)$H(X,Y,Z)$实际上应该理解为$H((X,Y,Z))$，即H总为一元函数

(2)$I(X;Y,Z)$实际上应该理解为$I(X;(Y,Z))$，即$I(\cdot;\cdot)$总为二元函数

(3)$H(X,Y|Z)$实际上应该理解为$H((X,Y)|Z)$，即$H(\cdot|\cdot)$总为二元函数；同理$I(X;Y|Z)$实际上应该理解为$I(X;Y|Z)$，即$I(\cdot;\cdot|\cdot)$总为三元函数。应该认为“；”的优先级高于“$|$”

(4)D虽称为熵，但不是随机变量的函数，而是分布的函数。"$\|$"类似于“；”，D总为二元函数

\begin{definition}[条件互信息]
    X,Y在已知Z的条件下的条件互信息(conditional mutual information)定义为
    \[I(X;Y|Z) = \sum_{z \in \mathcal{Z}} p(z) \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y|z) \log \frac{p(x,y|z)}{p(x|z)p(y|z)} = \mathbb{E} _p\left[\log \frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}\right]\]
\end{definition}
\begin{remark}
    描述已知Z，给出Y引起的X的不确定度减少量
\end{remark}

\begin{definition}[条件相对熵]
    对于联合概率质量函数$p(x,y)$和$q(x,y)$，X在已知Y的条件下的条件相对熵(conditional relative entropy)定义为
    \[D(p(x|y)||q(x|y)) = \sum_{y \in \mathcal{Y}} p(y) \sum_{x \in \mathcal{X}} p(x|y) \log \frac{p(x|y)}{q(x|y)} = \mathbb{E} _p\left[\log \frac{p(X|Y)}{q(X|Y)}\right]\]
\end{definition}

\begin{theorem}[熵的链式法则]
    \[H(X_1,X_2,\ldots,X_n) = \sum_{i=1}^n H(X_i|X_{1},X_{2},\ldots,X_{i-1})\]
\end{theorem}
\begin{proof}
    反复使用
    \[H(X,Y) = H(X) + H(Y|X)\]
\end{proof}

\begin{theorem}[互信息的链式法则]
    \[I(X;Y_1,Y_2,\ldots,Y_n) = \sum_{i=1}^n I(X;Y_i|Y_{1},Y_{2},\ldots,Y_{i-1})\]
\end{theorem}
\begin{proof}
    \[I(\mathbf{X};\mathbf{Y}) = H(\mathbf{Y}) - H(\mathbf{Y}|\mathbf{X})\]
\end{proof}

\begin{theorem}[相对熵的链式法则]
    \[D(p(x,y)||q(x,y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x))\]
\end{theorem}
\begin{proof}
    \begin{align*}
        D(p(x,y)||q(x,y)) &= \mathbb{E} _{p(x,y)}\log \frac{p(X,Y)}{q(X,Y)} \\
        &= \mathbb{E} _{p(x,y)}\log \frac{p(X)p(Y|X)}{q(X)q(Y|X)} \\
        &= \mathbb{E} _{p(x)}\log \frac{p(X)}{q(X)} + \mathbb{E} _{p(x,y)}\log \frac{p(Y|X)}{q(Y|X)} \\
        &= D(p(x)||q(x)) + D(p(y|x)||q(y|x))
    \end{align*}
\end{proof}

\section{不等式}

\begin{theorem}[Jensen不等式]
    设X为离散型随机变量，取值（样本空间）为$\mathcal{X}$，概率分布为$p(x)$，且$f$为凸函数，则有
    \[\mathbb{E} _p[f(X)] \geq f(\mathbb{E} _p[X])\]
    进一步，若f为严格凸函数，则等号成立$\iff X=\mathbb{E}X$
\end{theorem}

\begin{theorem}[信息不等式]
    设p和q为定义在同一样本空间$\mathcal{X}$上的两个离散概率分布，则有
    \[D(p||q) \geq 0\]
    取等$\iff p=q$    
\end{theorem}
\begin{proof}
    $\log$在$(0,+\infty)$上为严格凹函数，故$-\log$为严格凸。考虑
    \[A=\{x\in \mathcal{X}: p(x)>0,q(x)>0\}\]
    则由Jensen不等式，
    \begin{align*}
        D(p||q) &= \sum_{x \in A} p(x) \log \frac{p(x)}{q(x)} \\
        &= -\sum_{x \in A} p(x) \left[-\log \frac{q(x)}{p(x)}\right] \\
        &\geq -\log \left(\sum_{x \in A} p(x) \frac{q(x)}{p(x)}\right) \\
        &= -\log \left(\sum_{x \in A} q(x)\right) \\
        &\geq 0
    \end{align*}
\end{proof}
\begin{corollary}
    \[I(X;Y) \geq 0\]
    取等$\iff X$与$Y$独立

    条件相对熵、条件互信息也是非负的
\end{corollary}
\begin{corollary}
    \[H(X|Y) \leq H(X)\]
    取等$\iff X$与$Y$独立
\end{corollary}
\begin{remark}
    条件导致熵减小。但$H(X|Y=y)$可能大于$H(X)$，不等式仅描述平均性质
\end{remark}

\begin{theorem}
    \[H(X) \leq \log |\mathcal{X}|\]
    取等$\iff X\sim U_{\mathcal{X} }$
\end{theorem}
\begin{corollary}[熵的独立界]
    \[H(X_1,X_2,\ldots,X_n) \leq \sum_{i=1}^n H(X_i)\]
    取等$\iff X_1,X_2,\ldots,X_n$相互独立
\end{corollary}
\begin{proof}
    由链式法则，
    \[H(X_1,X_2,\ldots,X_n) = \sum_{i=1}^n H(X_i|X_{1},X_{2},\ldots,X_{i-1}) \leq \sum_{i=1}^n H(X_i)\]
\end{proof}

\section{其他背景下的不等式}

\begin{definition}
    如果Z的条件分布$p(z|x,y)$仅依赖于y而与x条件独立，即
    \[p(x,y,z) = p(x)p(y|x)p(z|y)\]
    则称随机变量三元组(X,Y,Z)构成马尔可夫链(Markov chain)，记为$X \to Y \to Z$

    条件独立的意思是
    \[p(x,z|y) = p(x|y)p(z|y)\]
    X,Z对称，即$X \to Y \to Z \iff Z \to Y \to X$，因此可记$X \leftrightarrow Y \leftrightarrow Z$
\end{definition}

\begin{theorem}[数据处理不等式]
    设随机变量三元组(X,Y,Z)构成马尔可夫链$X \to Y \to Z$，则有
    \[I(X;Y) \geq I(X;Z)\]
    取等$\iff X \to Z \to Y$
\end{theorem}
\begin{proof}
    用互信息的链式法则，
    \[I(X;Y,Z) = I(X;Y) + I(X;Z|Y) = I(X;Z) + I(X;Y|Z)\]
    给定Y，X与Z独立，即$I(X;Z|Y)=0$，而$I(X;Y|Z)\geq 0$，故
    \[I(X;Y) \geq I(X;Z)\]
\end{proof}
\begin{remark}
    对Y的数据处理不能增加其包含X的信息
\end{remark}
\begin{corollary}
    \[I(X;Y)\geq I(X,g(Y))\]
    \[X \to Y \to Z\implies I(X;Y|Z)\leq I(X;Y)\]
\end{corollary}
\begin{remark}
    观察下游随机变量，X,Y的依赖程度可能降低
\end{remark}

\begin{definition}[充分统计量]
    假定有一族概率质量函数$\{f_{\theta}(x)\}$，X是从其中一个分布$f_{\theta}(x)$中抽取的样本，T(X)是X的一个统计量，则
    \[\theta \to X \to T(X)\]
    由数据处理不等式，有
    \[I(\theta;X) \geq I(\theta;T(X))\]
    取等时统计量T(X)未损失X关于参数$\theta$的信息，称T(X)为关于分布族$\{f_{\theta}(x)\}$的充分统计量(sufficient statistic)

    等价定义：给定T(X)，X与$\theta$条件独立，即$\theta \to T(X) \to X$
\end{definition}

\begin{definition}[最小充分统计量]
    关于分布族$\{f_{\theta}(x)\}$的充分统计量T(X)是其他任何充分统计量$U(X)$的函数，则称T(X)为关于$\{f_{\theta}(x)\}$的最小充分统计量(minimal sufficient statistic)
    
    定义蕴含$\theta \to T(X) \to U(X) \to X$
\end{definition}
\begin{remark}
    最小充分统计量最大程度地压缩了样本X中关于$\theta$的信息，而其他充分统计量可能包含冗余信息
\end{remark}

由随机变量Y估计与之有关的X，X的估计值记为$\hat{X}=g(Y)$取值空间为$\hat{\mathcal{X}}$，则有马尔可夫链$X \to Y \to \hat{X}$。定义误差概率$P_e = P\{\hat{X} \neq X\}$。
\begin{theorem}[Fano不等式]
    设X为离散型随机变量，取值（样本空间）为$\mathcal{X}$，则有
    \[H(P_e) + P_e \log (|\mathcal{X}| - 1) \geq H(X|\hat{X})\geq H(X|Y)\]
    可减弱为
    \[1+P_e \log |\mathcal{X}|\geq H(X|Y)\implies P_e \geq \frac{H(X|Y)-1}{\log |\mathcal{X}|}\]
\end{theorem}
\begin{proof}
    设错误指示变量$E=\mathds{1} _{\{\hat{X} \neq X\}}$，则有马尔可夫链$X \to Y \to \hat{X} \to E$。由链式法则，
    \begin{align*}
        H(X,E|\hat{X}) &= H(E|\hat{X}) + H(X|E,\hat{X}) \\
        &= H(X|\hat{X}) + H(E|X,\hat{X})
    \end{align*}
    因为$H(E|X,\hat{X})=0$，所以
    \[H(X|\hat{X}) = H(E|\hat{X}) + H(X|E,\hat{X})\]
    注意到
    \[H(E|\hat{X}) \leq H(E) = H(P_e)\]
    且
    \[H(X|E,\hat{X}) = P_e H(X|\hat{X},E=1) + (1-P_e)H(X|\hat{X},E=0) \leq P_e \log (|\mathcal{X}| - 1)\]
    故
    \[H(X|\hat{X}) \leq H(P_e) + P_e \log (|\mathcal{X}| - 1)\]
    另一方面，由数据处理不等式，
    \[H(X|Y) \leq H(X|\hat{X})\]
\end{proof}
\begin{corollary}
    令$\hat{X}=Y$，则有
    \[H(P_e) + P_e \log (|\mathcal{X}| - 1) \geq H(X|Y)\]
    若$\hat{X}=X$，结论变为
    \[H(P_e) + P_e \log (|\mathcal{X}| - 1) \geq 0\]
\end{corollary}

\begin{proposition}
    设X,X'独立同分布，则
    \[P\{X=X'\}\geq 2^{-H(X)}\]
\end{proposition}
\begin{corollary}
    设X,Y独立，$X\sim p(x)$，$Y\sim q(y)$，取值空间均为$\mathcal{X}$，则
    \[P\{X=Y\}\geq 2^{-H(p)-D(p\|q)},P\{X=Y\}\geq 2^{-H(q)-D(q\|p)}\]
\end{corollary}

\chapter{渐进均分性}

\section{渐进均分性}

\begin{definition}[随机变量的收敛]
    给定随机变量序列$\{X_n\}$和随机变量X，

    \ding{172}如果$$\forall \epsilon >0,P\{|X_n - X| \geq \epsilon\} \to 0(n\to \infty)$$
    则称$X_n$依概率收敛于X，记为$X_n \xrightarrow{P} X$

    \ding{173}如果$$P\{\lim_{n\to \infty} X_n = X\} = 1$$
    则称$X_n$几乎处处收敛（或以概率1收敛）于X，记为$X_n \xrightarrow{a.e.} X$

    \ding{174}如果$$\mathbb{E} (X_n-X)^2 \to 0(n\to \infty)$$
    则称$X_n$均方收敛于X，记为$X_n \xrightarrow{L_2} X$
\end{definition}

\begin{theorem}[渐进均分性(Asymptotic Equipartition Property, AEP)]
    以X记信源随机变量，它生成的序列$X_1,X_2,\ldots,X_n$i.i.d.$\sim p(x)$，则有
    \[\ -\frac{1}{n} \log p(X_1,X_2,\ldots,X_n) \xrightarrow{P} H(X)\]
\end{theorem}
\begin{proof}
    \[X_k\ i.i.d.\sim p(x)\implies -\log p(X_k)\ i.i.d.\sim -\log p(x)\]
    由弱大数定律，
    \[\frac{1}{n} \sum_{k=1}^n -\log p(X_k) \xrightarrow{P} \mathbb{E} [-\log p(X)] = H(X)\]
\end{proof}

\begin{definition}[典型集]
    关于p(x)的典型集(typical set)定义为
    \[A_{\epsilon}^{(n)} = \left\{(x_1,x_2,\ldots,x_n)\left|-\frac{1}{n} \log p(x_1,x_2,\ldots,x_n) - H(X)\right. < \epsilon\right\}\]
    性质：
    
    \ding{172} $$\forall \mathbf{x}\in A_{\epsilon}^{(n)},H(X)-\epsilon < -\frac{1}{n} \log p(\mathbf{x}) < H(X)+\epsilon$$

    \ding{173} n充分大时，$P\{A_{\epsilon}^{(n)}\} > 1-\epsilon$

    \ding{174} $|A_{\epsilon}^{(n)}| \leq 2^{n(H(X)+\epsilon)}$（概率和不超过1）

    \ding{175} （第一条的推论）n充分大时，$|A_{\epsilon}^{(n)}| \geq (1-\epsilon)2^{n(H(X)-\epsilon)}$
\end{definition}
\begin{remark}
    直观：

    \ding{172}$\implies$典型集中的元素在数量级意义下是几乎等可能的；

    \ding{173}$\implies$典型集出现概率随n增大而趋近于1（渐进）；

    \ding{174}\ding{175}$\implies$典型集的元素个数近似等于$2^{nH(X)}$（均分）
\end{remark}

设$X_n\ i.i.d.\sim p(x)$，存在一个编码将长为n的序列映射为比特串，且映射为双射（从而可逆），其码字长度$l(x_n)$满足
\[\lim_{n\to\infty}\mathbb{E} \left[ \frac{1}{n}l(x_n)\right]=H(X)\]
因而理论上用$nH(X)$比特即可表示序列$x_1,x_2,\ldots,x_n$（等长码需要$n\log |\mathcal{X}|$比特）

码字长度：信源编码时某个符号$x$使用的比特数为$l(x)$；$x^n$使用的比特数为$l(x^n)$

\begin{proof}
    考虑给大概率的典型集较短的编码。
    \[A_{\epsilon}^{(n)}\leq 2^{n(H(X)+\epsilon)}\implies \lceil n(H(X)+\epsilon) \rceil\leq n(H(X)+\epsilon)+1(bit)\]
    可表示$A_{\epsilon}^{(n)}$中的每个序列，同理$n\log|\mathcal{X} |+1$bit可表示$A_{\epsilon}^{(n)c}$

    在典型集序列前标0而在非典型集序列前标1作为表示为，则码字长度
    \[l(x^n)\leq\begin{cases}
        n(H(X)+\epsilon)+2, & x^n\in A_{\epsilon}^{(n)} \\
        n\log |\mathcal{X}| + 2, & x^n\in A_{\epsilon}^{(n)c}
    \end{cases}\]
    
    取n充分大使$P\{A_{\epsilon}^{(n)}\} > 1-\epsilon$，则
    \begin{align*}
        \mathbb{E} [l(x^n)]&=\sum_{x^n\in \mathcal{X} ^n} p(x^n) l(x^n)\\
        &\leq P\{A_{\epsilon}^{(n)}\} [n(H(X)+\epsilon)+2] + P\{A_{\epsilon}^{(n)c}\} [n\log |\mathcal{X}| + 2] \\
        &\leq (1-\epsilon) [n(H(X)+\epsilon)+2] + \epsilon [n\log |\mathcal{X}| + 2]=n(H(X)+\epsilon') 
    \end{align*}
    其中$\epsilon' = \epsilon \log |\mathcal{X}| - \epsilon H(X) + 2/n$

    \begin{align*}
        \mathbb{E} [l(x^n)]&\geq P\{A_{\epsilon}^{(n)}\} [n(H(X)+\epsilon)+1]+P\{A_{\epsilon}^{(n)c}\} [n\log |\mathcal{X}| + 1] \\
        &\geq (1-\epsilon) [n(H(X)+\epsilon)+1]+ \epsilon [n\log |\mathcal{X}| + 1] = n[(1-\epsilon)H(X)+\epsilon'']
    \end{align*}
    其中$\epsilon'' =\epsilon(1-\epsilon)+\frac{1-\epsilon}{n}$
\end{proof}
\begin{remark}
    熵是无损压缩的下限
\end{remark}

\chapter{熵率与Markov链}

\section{熵率}

\begin{definition}[熵率]
    设$\{X_n\}$为随机过程，则$\{X_n\}$的熵率(entropy rate)在极限存在时定义为
    \[H(\mathcal{X} ) = \lim_{n\to \infty} \frac{1}{n} H(X_1,X_2,\ldots,X_n)\]
\end{definition}

\begin{theorem}\label{thm:entropy_rate}
    定义
    \[H'(\mathcal{X} )= \lim_{n\to \infty} H(X_n|X_{1},X_{2},\ldots,X_{n-1})\]
    对于平稳过程，两种极限均存在且$H(\mathcal{X} ) = H'(\mathcal{X} )$
\end{theorem}
\begin{proof}
    \[H(X_{n+1}|X_{1},X_{2},\ldots,X_{n}) \leq H(X_{n+1}|X_{2},X_{3},\ldots,X_{n+1})=H(X_n|X_{1},X_{2},\ldots,X_{n-1})\]
    因此$H(X_n|X_{1},X_{2},\ldots,X_{n-1})$非负递减，$H'(\mathcal{X})$存在。

    由链式法则，
    \[\frac{1}{n}H(X_1,X_2,\ldots,X_n) = \frac{1}{n}\sum_{i=1}^n H(X_i|X_{1},X_{2},\ldots,X_{i-1})\]
    即熵率为条件熵的时间平均值。$H_n(\mathcal{X} )$为$H'(\mathcal{X} )$的Cesàro和，故$H(\mathcal{X} ) = H'(\mathcal{X} )$
\end{proof}

\section{Markov链}

\begin{theorem}
    对于平稳的Markov链$\{X_n\}$，设平稳分布为$\mu$，转移概率矩阵为P，则熵率为
    \[H(\mathcal{X} ) = H(X_2|X_1) = \sum_{i} \mu_i \sum_{j} P_{ij} \log \frac{1}{P_{ij}}\]
\end{theorem}
\begin{proof}
    \begin{align*}
        H'(\mathcal{X} ) &= \lim_{n\to \infty} H(X_n|X_{1},X_{2},\ldots,X_{n-1}) \\
        &= \lim_{n\to \infty} H(X_n|X_{n-1}) \\
        &= H(X_2|X_1) \\
        &= \sum_{i} \mu_i \sum_{j} P_{ij} \log \frac{1}{P_{ij}}
    \end{align*}
\end{proof}

\begin{theorem}
    设$\mu_n,\mu'_n$为n时刻同一Markov链的两条轨迹的分布，则
    \[D(\mu_n||\mu'_n) \geq D(\mu_{n+1}||\mu'_{n+1})\]
    特别地，
    \[D(\mu_n||\mu)\geq D(\mu_{n+1}||\mu)\]
\end{theorem}
\begin{proof}
    记$\mu_n,\mu'_n$对应两盒概率分布为$p,q$，$r(\cdot|\cdot)$为转移概率分布，则
    \[p(x_n,x_{n+1}) = p(x_n)r(x_{n+1}|x_n),q(x_n,x_{n+1}) = q(x_n)r(x_{n+1}|x_n)\]
    由相对熵的链式法则，\begin{align*}
        &D(p(x_n)||q(x_n))\\
        =& D(p(x_n)||q(x_n)) + D(r(x_{n+1}|x_n)||r(x_{n+1}|x_n)) \\
        =& D(p(x_{n+1})||q(x_{n+1})) + D(p(x_n|x_{n+1})||q(x_n|x_{n+1})) 
    \end{align*}

    由于$D(r||r)=0,D(p||q)\geq 0$，因此$D(p(x_n)||q(x_n)) \geq D(p(x_{n+1})||q(x_{n+1}))$，即$D(\mu_n||\mu'_n) \geq D(\mu_{n+1}||\mu'_{n+1})$。
\end{proof}
\begin{remark}
    相同转移概率下，不同初始分布趋同于平稳分布
\end{remark}
\begin{corollary}
    若平稳分布$\mu$为均匀分布，则熵增大，即$H(\mu_n)\leq H(\mu_{n+1})$
\end{corollary}
\begin{proof}
    \begin{align*}
        D(\mu_n||\mu) &= \sum_{x_n} \mu_n(x_n) \log \frac{\mu_n(x_n)}{1/|\mathcal{X}|} \\
        &=\log|\mathcal{X} | - H(\mu_n) \\
        &=\log |\mathcal{X}|-H(X_n)
    \end{align*}
\end{proof}
\begin{remark}
    热学中的熵$S=\log \Omega$在各状态等可能的前提下与信息熵一致，这解释了热二律“熵增大”
\end{remark}

\begin{definition}
    转移概率矩阵P的行和为1：$\sum_{j} P_{ij} = 1$，且$P_{ij}\geq 0$。若P列和也为1：$\sum_{i} P_{ij} = 1$，则P是双随机矩阵(doubly stochastic matrix)

    均匀分布是平稳分布$\iff$P双随机
\end{definition}

\begin{theorem}
    对于平稳的Markov链$\{X_n\}$，$H(X_{n+1}|X_1)\geq H(X_n|X_1)$
\end{theorem}
\begin{proof}
    \[H(X_{n+1}|X_1) \geq H(X_{n+1})|X_1,X_2=H(X_{n+1})|X_2=H(X_n|X_1)\]
\end{proof}

\begin{lemma}
    设$\{X_n\}$为平稳的Markov链，$\{Y_n\}$为$\{X_n\}$的对应项的函数，熵率为$H(\mathcal{Y} )$，则
    $$H(Y_n|Y_{n-1},\cdots,Y_1,X_1) \leq H(\mathcal{Y} )$$
\end{lemma}
\begin{proof}
    \begin{align*}
        H(Y_n|Y_{n-1},\cdots,Y_1,X_1) &\leq H(Y_n|Y_{n-1},\cdots,Y_1,X_1,X_0,\cdots,X_{-k}) &(\text{Markov性})\\
        &=H(Y_n|Y_{n-1},\cdots,Y_1,\cdots,Y_{-k},X_1,X_0,\cdots,X_{-k}) &(Y_k=\phi_k(X_k))\\
        &\leq H(Y_n|Y_{n-1},\cdots,Y_1,\cdots,Y_{-k})\\
        &=H(Y_{n+k+1}|Y_{n+k},\cdots,Y_{1})&(\text{平稳性}))\\
    \end{align*}
    令$k\to \infty$，则
    \[H(Y_n|Y_{n-1},\cdots,Y_1,X_1) \leq \lim_{k\to \infty} H(Y_{n+k+1}|Y_{n+k},\cdots,Y_{1}) = H(\mathcal{Y} )\]
\end{proof}

\begin{lemma}
    \[H{Y_n|Y_{n-1},\cdots,Y_1}-H(Y_n|Y_{n-1},\cdots,Y_1,X_1)\to 0,n\to\infty\]
\end{lemma}
\begin{proof}
\begin{align*}
    H(X_1)&\geq\lim_{n\to \infty} I(X_1;Y_n|Y_{n-1},\cdots,Y_1) \\
    &=\sum_{n=1}^{\infty}I(X_1;Y_n|Y_{n-1},\cdots,Y_1)&(\text{链式法则})\\
    &= \sum_{n=1}^{\infty} [H(Y_n|Y_{n-1},\cdots,Y_1)-H(Y_n|Y_{n-1},\cdots,Y_1,X_1)] 
\end{align*}
\end{proof}
又由\ref{thm:entropy_rate}，
\[\{X_n\}\text{平稳}\implies H(X_n|X_{1},X_{2},\ldots,X_{n-1})\downarrow H(\mathcal{X} )\]
\begin{theorem}
    设$\{X_n\}$为平稳的Markov链，$\{Y_n\}$为$\{X_n\}$的对应项的函数，熵率为$H(\mathcal{Y} )$，则
    \[H(Y_n|Y_{n-1},\cdots,Y_1,X_1)\leq H(\mathcal{Y} )\leq H(Y_n|Y_{n-1},\cdots,Y_1)\]
    且
    \[\lim_{n\to \infty} H(Y_n|Y_{n-1},\cdots,Y_1,X_1)=H(\mathcal{Y} ) = \lim_{n\to \infty} H(Y_n|Y_{n-1},\cdots,Y_1)\]
\end{theorem}

\chapter{数据压缩}

\section{编码理论}

\section{Huffman编码}

\chapter{信道容量}

\section{通信系统模型}

\begin{definition}[信源模型]
    (1)根据信源输出信号所对应的随机过程是否平稳，分为稳恒（平稳）信源和非稳恒（非平稳）信源

    (2)根据特殊的随机过程类型，分为高斯信源、Markov信源等

    (3)信源字母表离散，信号取值时刻离散的稳恒信源称为离散稳恒信源
\end{definition}

\begin{definition}[信道模型]
    (1)按输入输出信号在幅值和时间上的取值分为离散信道（数字信道）、连续信道等\\
    离散信道(discrete channel)是至多可数的输入字母表$\mathcal{X}$和输出字母表$\mathcal{Y}$，及$\mathcal{X} $到$\mathcal{Y} $的转移概率模型构成的系统

    (2)如果信道输出值域信道在该时刻的输入有关，二与先前的输入输出等无关，则信道是无记忆信道(memoryless channel)；否则为有记忆信道

    (3)按输入输出信号之间的关系是否确定，分为有噪信道和无噪信道等
\end{definition}

\begin{definition}[离散无记忆信道]
    离散无记忆信道(DMC)的转移概率模型可以用转移概率分布$p(y|x)$描述。
    
    设输入字母表为$\mathcal{X} $，输出字母表为$\mathcal{Y} $，该DMC可以用$$(\mathcal{X} ,p(x|y),\mathcal{Y} )$$表示
    
    离散无记忆信道的n次扩展可以用$$(\mathcal{X} ^n,p(x^n|y^n),\mathcal{Y} ^n)$$表示，其中
    \[p(y_k|x^k,y^{k-1})=p(y_k|x_k),\forall k\in\{1,2,\cdots,n\}\]

    进一步，如果信道不带反馈（如不明确指出，则默认不带反馈），即输入字符不依赖于过去的输出字符，即
    $$p(x_k|x^{k-1},y^{k-1})=p(x_k|x^{k-1}),\forall k$$
    则n次扩展的信道转移函数简化为
    \[p(y^k|x^k)=\prod_{i=1}^k p(y_i|x_i),\forall k\]
\end{definition}

\begin{definition}[信息信道容量]
    离散无记忆信道的信息信道容量(information channel capacity)定义为
    \[C = \max_{p(x)} I(X;Y)\]
\end{definition}
香农第二定理将指出信息信道容量=信道容量=信道最高码率=任意小误差传输比特数/信道使用次数。

$I(X;Y)$关于输入分布$p(x)$是紧集上的连续的凹函数，因此最大值存在且唯一，使用$\max$是合理的。

\begin{proposition}[信息信道容量的性质]
    \quad
    \begin{itemize}
        \item $C\geq 0$，取等$\iff$信道输出与输入独立
        \item $C\leq \log |\mathcal{X}|$，取等$\iff X,Y$独立且$X\sim U_{\mathcal{X} }$
        \item $C\leq \log |\mathcal{Y}|$，取等$\iff$存在输入分布使得输出分布为均匀分布
    \end{itemize}
\end{proposition}

\begin{definition}[对称信道]
    如果离散无记忆信道的概率转移矩阵$p(x|y)$是数独矩阵，则称该信道为对称信道(symmetric channel)。数独矩阵必然是双随机的。

    数独矩阵可以描述为：任何两行互为置换、任何两列也互为置换。
    
    如果每一行$p(\cdot|x)$都是同一分布的置换，且所有列的元素和$\sum_{x}p(y|x)$相同，则该信道是弱对称信道(weakly symmetric channel)
\end{definition}

\begin{theorem}
    对于弱对称信道，信息信道容量为
    \[C = \log |\mathcal{Y}| - H(p(\cdot|x))\]
    其中$p(\cdot|x)$为任一行的分布。取等$\iff$输入分布为均匀分布
\end{theorem}

\begin{example}[有噪打字机信道]
    该种信道的概率转移矩阵矩阵的基本特征是，对角线元素（不出错的概率）接近于1，除了对角线，每一行都有个别元素数值较大，而其他元素极小或为0。描述了考虑失误的打字机。

    输入、输出字母表相同，输入以0.5的概率被输出端无改变地接收，以0.5的概率变为下一个字母。字母表大小为26时，$C=13 bit/$传输：

    一方面，每次传输可以无误差地传输其中13个字符（仅传输奇数位置或偶数位置的字符）；

    另一方面，$$C=\max I(X;Y)=\max [H(Y)-H(Y|X)]=\max H(Y)-1=\log 26-1=\log 13 bit/\text{传输}$$

    取等$\iff p(x)\sim U$
\end{example}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{communicatioin_channel.jpg}
    \caption{离散通信信道模型}
\end{figure}

\begin{definition}[$(M,n)$码]
    信道$(\mathcal{X} ,p(y|x),\mathcal{Y} )$的$(M,n)$码由以下部分构成：\begin{itemize}
        \item 指标集$\{1,2,\cdots,M\}$
        \item 编码函数(encoding function)
        $$x^n:\{1,2,\cdots,M\}\to \mathcal{X} ^n$$
        生成码字(codewords)$x^n(1),x^n(2),\cdots,x^n(M)$，所有码字的集合称为码本(code-book)$$\mathcal{C}=\{x^n(1),x^n(2),\cdots,x^n(M)\}=\{\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_M\}$$
        \item 译码函数
        $$g:\mathcal{Y} ^n\to\{1,2,\cdots,M\}$$
    \end{itemize}
\end{definition}

\begin{definition}[条件误差概率]
    已知指标$i$被发送，（给定发送符号时的）条件误差概率(conditional probability of error)为
    \[\lambda_i=P\{g(Y^n)\neq i|X^n=x^n(i)\}\]
    它衡量了特定符号在传输过程中的脆弱性或抗干扰能力。

    已知接收到符号y，（给定接收符号时的）条件误差概率为
    \[1-\max_{x}P\{X=x|Y=y\}\]
    反映了收到特定y后，接收端对这次判决的不确定性。条件误差概率通常指前者。
\end{definition}

\begin{definition}
    $(M,n)$码的最大误差概率(maximun probability of error)为
    \[\lambda^{(n)}=\max_{i\in\{1,2,\cdots,M\}} \lambda_i\]

    （算数）平均误差概率(average probability of error)为
    \[P^{(n)}_e=\overline{\lambda}=\frac{1}{M}\sum_{i=1}^{M}\lambda_i\]
\end{definition}

\begin{remark}
    如果指标W的选取服从均匀分布，则$P^{(n)}_e=P\{W\neq g(Y^n)\}$。
\end{remark}
\begin{remark}
    显然，$P_e^{(n)}\leq \lambda^{(n)}$。通常希望这二者差异大，但将证明：在相同码率下，平均误差概率小$\implies$最大误差概率小
\end{remark}

\begin{definition}[码率]
    $(M,n)$码的码率(rate)为
    \[R=\frac{\log M}{n}(bit/\text{传输})\]
    如果存在一个$(\lceil 2^{nR}\rceil,n)$码，满足$\lambda^{(n)}\to 0,n\to 0$，则称码率R是可达的(achievable)。一般将此编码简记为$(2^{nR},n)$。

    信道容量定义为可达码率的上确界。
\end{definition}

\begin{definition}[联合典型序列]
    服从分布$p(x,y)$的联合典型序列(jointly typical sequence)$\{(x^n,y^n)\}$所构成的集合$A^{(n)}_{\epsilon}$是指其经验熵与真实熵以$\epsilon$为界靠近的n长序列构成的集合，即：
    \begin{align*}
        A^{(n)}_{\epsilon}=\left\{(x^n,y^n)\in \mathcal{X}^n\times \mathcal{Y}^n:\right.
        &\left|-\frac{1}{n}\log p(x^n)-H(X)\right|<\epsilon,\\
        &\left|-\frac{1}{n}\log p(y^n)-H(Y)\right|<\epsilon,\\
        &\left.\left|-\frac{1}{n}\log p(x^n,y^n)-H(X,Y)\right|<\epsilon\right\}
    \end{align*}
    其中
    \[p(x^n,y^n)=\prod_{i=1}^{n}p(x_i,y_i)\]
\end{definition}

\end{document}